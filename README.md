# ğŸ™ï¸ Voice RAG Agent

Real-time voice AI agent where you can talk over WebRTC using LiveKit and get answers powered by Retrieval-Augmented Generation (RAG) over your uploaded documents â€” during the call.

[![Ask DeepWiki](https://deepwiki.com/badge.svg)](https://deepwiki.com/ManojKumarKarumanchi/voice-rag-agent)
---
## ğŸš€ Features

* **Voice over WebRTC** â€“ Real-time voice conversation via LiveKit
* **Speech-to-Text (STT)** â€“ Groq Whisper (free tier)
* **LLM** â€“ Groq LLaMA models (free tier)
* **Text-to-Speech (TTS)** â€“ Cartesia (optional) or text-only mode
* **RAG Pipeline** â€“ Upload PDF, CSV, TXT, MD files
* **Vector Search** â€“ FAISS-based semantic retrieval
* **Modern Frontend** â€“ React + Vite
* **Live Transcript & Sources Panel** â€“ See retrieved chunks in real time
* **Editable System Prompt** â€“ Modify agent behavior on the fly

---

## ğŸ— Architecture

```mermaid
flowchart TD
    A["React UI (Frontend)\n- WebRTC connect\n- Edit prompt\n- Upload files"]
    B["LiveKit Room (WebRTC Audio Stream)"]
    C["FastAPI Backend\n- Upload Files\n- FAISS RAG\n- Token Generation"]
    D["STT (Groq Whisper optional)"]
    E["LLM (Groq Llama optional)"]
    F["TTS (Cartesia optional)"]

    A -->|WebRTC| B
    B -->|WebRTC| D
    A -->|REST / HTTP| C
    C -->|HTTP| D
    D --> E
    E --> F
```

---

## ğŸ§  Embedding & RAG Setup

We use:

* **Sentence Transformers**
* **LangChain RecursiveCharacterTextSplitter**
* **FAISS vector store**
* **PyPDF fallback loader**

```python
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter

embed_model = SentenceTransformer("BAAI/bge-small-en")

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
```

### Embedding Model

* `BAAI/bge-small-en`
* Lightweight and optimized for semantic retrieval
* Ideal for local + production RAG setups

---

## ğŸ›  Tech Stack

### Backend

* FastAPI
* FAISS (vector search)
* Sentence Transformers
* LangChain text splitters
* PyPDF (fallback document loader)

### Real-Time Layer

* LiveKit (WebRTC audio rooms)

### AI Services

* Groq (Whisper STT + LLaMA inference)
* Cartesia (optional TTS)

### Frontend

* React
* Vite
* WebRTC

---

## ğŸ“‹ Prerequisites

* Python 3.12+
* Node.js 18+
* LiveKit Cloud account (free tier available)
* Groq API key (free tier available)

---

# âš™ï¸ Installation & Running

You must run **3 services**:

1. Backend
2. Agent
3. Frontend

---

## ğŸªŸ Windows 11

Open **3 separate terminals**:

### Terminal 1 â€“ Backend

```powershell
cd C:\path\to\voice-rag-agent

python -m venv .venv
.venv\Scripts\activate

pip install -r requirements.txt
python backend\main.py
```

### Terminal 2 â€“ Agent

```powershell
cd C:\path\to\voice-rag-agent\agent
python agent.py dev
```

### Terminal 3 â€“ Frontend

```powershell
cd C:\path\to\voice-rag-agent\frontend

npm install
npm run dev
```

---

## ğŸ§ Linux / macOS

Open **3 separate terminals**:

### Terminal 1 â€“ Backend

```bash
cd /path/to/voice-rag-agent

python3 -m venv .venv
source .venv/bin/activate

pip install -r requirements.txt
python3 backend/main.py
```

### Terminal 2 â€“ Agent

```bash
cd /path/to/voice-rag-agent/agent
source ../.venv/bin/activate
python3 agent.py dev
```

### Terminal 3 â€“ Frontend

```bash
cd /path/to/voice-rag-agent/frontend
npm install
npm run dev
```

---

# ğŸ” Environment Setup

Create `.env` file inside `backend/`:

```env
# LiveKit
LIVEKIT_URL=wss://your-project.livekit.cloud
LIVEKIT_API_KEY=your_api_key
LIVEKIT_API_SECRET=your_api_secret

RAG_BACKEND_URL=http://localhost:8000

# AI Services
GROQ_API_KEY=your_groq_key
CARTESIA_API_KEY=your_cartesia_key
```

---

# ğŸ“‚ Project Structure

```
VOICE-RAG-AGENT/
â”‚
â”œâ”€â”€ agent/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ agent.py
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ rag.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api.js
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â””â”€â”€ VoiceRoom.jsx
â”‚   â”‚
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.js
â”‚
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ sample-doc.txt
```

---

# ğŸ”Œ API Endpoints

| Endpoint     | Method | Description              |
| ------------ | ------ | ------------------------ |
| `/upload`    | POST   | Upload document for RAG  |
| `/ragStatus` | GET    | Get indexing status      |
| `/retrieve`  | POST   | Retrieve relevant chunks |
| `/getToken`  | POST   | Generate LiveKit token   |
| `/health`    | GET    | Health check             |

---

# â–¶ï¸ Usage

1. Open `http://localhost:3000`
2. Upload documents (PDF, CSV, TXT, MD supported)
3. Wait for "Document is ready"
4. Click **Start Call**
5. Click **Start Talking**
6. Ask questions â€” see:

   * Live transcript
   * Retrieved sources
   * AI response (text or voice)

---

# ğŸ” How It Works

1. **Upload Document**
   Files are parsed (PyPDF fallback supported).

2. **Chunking & Embedding**

   * Split into overlapping chunks
   * Embedded using BGE model
   * Stored in FAISS

3. **Voice Interaction Flow**

   * Speech â†’ Groq Whisper â†’ Text
   * Text â†’ FAISS Retrieval
   * Retrieved Chunks + Query â†’ Groq LLM
   * Response â†’ Transcript + Optional TTS

---

# ğŸ›  Troubleshooting

**No transcript showing**

* Ensure agent is running:

  ```bash
  python agent.py dev
  ```

**Documents not indexing**

* Check backend console logs
* Verify file format support

**Connection issues**

* Confirm LiveKit credentials in `.env`
* Ensure backend is running on port 8000

---
Demo: https://tinyurl.com/voicerag
---
# ğŸ“œ License

MIT
